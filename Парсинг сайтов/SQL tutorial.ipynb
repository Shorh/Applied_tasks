{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Список глав + распределение по частям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers(base_url, next_page):\n",
    "    header_list = []\n",
    "    \n",
    "    while next_page:\n",
    "        current_url = base_url + next_page\n",
    "        page = urllib.request.urlopen(current_url)\n",
    "        soup = BeautifulSoup(page)\n",
    "\n",
    "        next_page = soup.select('.part_pages_sel + td')\n",
    "        if next_page:\n",
    "            next_page = next_page[0].find('a').get('href')\n",
    "        else:\n",
    "            try:\n",
    "                next_page = soup.findAll('label', {'class': 'nav_label2'})[1].find('a').get('href')\n",
    "            except IndexError:\n",
    "                next_page = None\n",
    "\n",
    "        content_text = soup.select('#content_text')\n",
    "        if content_text:\n",
    "            header_list.append(soup.select('.cttl')[0].text)\n",
    "#             print(soup.select('.cttl')[0].text)\n",
    "        \n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://www.sql-tutorial.ru'\n",
    "next_page = '/ru/book_typical_mistakes_select.html'\n",
    "\n",
    "header_list = headers(base_url, next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./SQL_tutorial_parts.html', 'w') as output_file:\n",
    "    for i in header_list:\n",
    "        output_file.write(i.replace('\\n', '') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_df = pd.DataFrame(header_list, columns=['chapter'])\n",
    "header_df = header_df.assign(part=np.array([2] * 120 + [3] * 72 + [4] * 59 + [5] * 36 + [6] * 49))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбор глав по частям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pars_html(base_url, next_page):\n",
    "    common_list = []\n",
    "    \n",
    "    with open(f'./SQL_tutorial_common.html', 'r') as input_file:\n",
    "        for line in input_file:\n",
    "            common_list.append(line)\n",
    "\n",
    "    while next_page:\n",
    "#     for i in range(1):\n",
    "        current_url = base_url + next_page\n",
    "        page = urllib.request.urlopen(current_url)\n",
    "        soup = BeautifulSoup(page)\n",
    "\n",
    "        next_page = soup.select('.part_pages_sel + td')\n",
    "        if next_page:\n",
    "            next_page = next_page[0].find('a').get('href')\n",
    "        else:\n",
    "            try:\n",
    "                next_page = soup.findAll('label', {'class': 'nav_label2'})[1].find('a').get('href')\n",
    "            except IndexError:\n",
    "                next_page = None\n",
    "\n",
    "        content_text = soup.select('#content_text')\n",
    "        if content_text:\n",
    "            header = soup.select('.cttl')[0].text\n",
    "            npage = header_df[header_df['chapter'] == header]['part'].iloc[0]\n",
    "            with open(f'./SQL_tutorial_part{npage}.html', 'a') as output_file:\n",
    "                output_file.write(str(common_list[0]))\n",
    "                output_file.write(str(soup.select('#parent_list')[0]) + '\\n')\n",
    "                output_file.write(str(soup.select('.cttl')[0]) + '\\n')\n",
    "                output_file.write(str(common_list[1]))\n",
    "                output_file.write(str(content_text[0])[:-6] + '\\n')\n",
    "\n",
    "                add_content = soup.html\n",
    "                if add_content.findNextSibling():\n",
    "                    while add_content.findNextSibling().name != 'noindex':\n",
    "                        if add_content.findNextSibling().name != 'div':\n",
    "                            output_file.write(str(add_content.findNextSibling()) + '\\n')\n",
    "                            add_content = add_content.findNextSibling()\n",
    "                            if add_content.findNextSibling() is None:\n",
    "                                print(soup)\n",
    "                                print('*'*80)\n",
    "                        else:\n",
    "                            break\n",
    "                output_file.write('</div>')\n",
    "                output_file.write(str(common_list[2]))\n",
    "                \n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://www.sql-tutorial.ru'\n",
    "next_page = '/ru/book_typical_mistakes_select.html'\n",
    "\n",
    "pars_html(base_url, next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
